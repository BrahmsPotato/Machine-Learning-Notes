{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    " \n",
    "### Classification Metrics Table\n",
    "指标|\t描述|\tScikit-learn函数|\n",
    ":----|:---|:---|\n",
    "Precision|\t精准度|\tfrom sklearn.metrics import precision_score|\n",
    "Recall|\t召回率|\tfrom sklearn.metrics import recall_score|\n",
    "F1|\tF1值|\tfrom sklearn.metrics import f1_score|\n",
    "Confusion Matrix|\t混淆矩阵|\tfrom sklearn.metrics import confusion_matrix|\n",
    "ROC\t|ROC曲线|\tfrom sklearn.metrics import roc|\n",
    "AUC|\tROC曲线下的面积|\tfrom sklearn.metrics import auc|\n",
    "\n",
    "### Regression Metrics Table\n",
    "指标|\t描述|\tScikit-learn函数|\n",
    ":-----|:-----|:---|\n",
    "Mean Square Error (MSE, RMSE)|\t平均方差|\tfrom sklearn.metrics import mean_squared_error|\n",
    "Absolute Error (MAE, RAE)|\t绝对误差|\tfrom sklearn.metrics import mean_absolute_error, median_absolute_error|\n",
    "R-Squared|\tR平方值|\tfrom sklearn.metrics import r2_score|\n",
    "### MAE, MSE, R-Squared & RMSE\n",
    "\n",
    "\n",
    "$$MAE=\\frac{1}{n}\\sum_{i=1}^{n}{|y_i-\\hat{y_i}|}$$\n",
    "\n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^{n}{(y_i-\\hat{y_i})^2}$$\n",
    "\n",
    "$$R^2=1-\\frac{\\sum_{i=1}^{n}{(y_i-\\hat{y_i})^2}}{\\sum_{i=1}^{n}{(y_i-\\bar{y})^2}}$$\n",
    "\n",
    "$$RMSE=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(y_i-\\hat{y_i})^2}}$$\n",
    "\n",
    "\n",
    "RMSE比MAE对大误差，尤其是极值非常敏感。出现你说的情况的原因是，虽然整体上误差绝对值在不断减小，但误差的分布发生了变化，两个尾部更厚了一点。你说的反相关是负相关吧，这两个一般是正相关的，但是显然能从散点图里面挑出来一小部分点，使得他们跟整体的规律相反。但是我觉得这不是负相关。\n",
    "\n",
    "$R^{2}$ 越大，越接近1，则表明解释变量$x_{i}$ 和预测变量$y_{i}$ 之间的相关性越强。\n",
    "\n",
    "### Confusion matrix\n",
    "Real \\ Predict | Postive | Negative |\n",
    ":-------------:|:-------:|:--------:|\n",
    "Postive        | TP      | FN       |\n",
    "Negative       | FP      | TN       |\n",
    "\n",
    "### Accuracy(准确率)\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + FN + FP + TN}$$\n",
    "\n",
    "\n",
    "### Precision (查准率)\n",
    "\n",
    "$$P = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "### Recall (查全率/召回率)\n",
    "\n",
    "$$R = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "### F1, Fn\n",
    "查准率和查全率往往是矛盾的度量，所以需要F度量(调和平均值)作为平衡的指标。与算术平均值$(\\frac{P+R}{2})$和几何平均值$(\\sqrt{P*R})$相比，调和平均值更重视较小值。多分类时可使用混淆矩阵的平均值或直接使用查准率P和召回率R的平均值。\n",
    "\n",
    "\n",
    "$$\\frac{1}{F1} = \\frac{1}{2} (\\frac{1}{P} + \\frac{1}{R})$$\n",
    "\n",
    "$$F1 = \\frac{2*P*R}{P + R}$$\n",
    "\n",
    "$$\\frac{1}{F_\\beta} = \\frac{1}{1+ \\beta^2} (\\frac{1}{P} + \\frac{\\beta^2}{R})$$\n",
    "\n",
    "$$F_\\beta = \\frac{(1+ \\beta^2)*P*R}{(\\beta^2 *P) + R}$$\n",
    "\n",
    "当$\\beta>0$时度量了查全率R对查准率P的重要性。$\\beta>1$时查全率R有更大影响，$\\beta<1$时查准率P有更大影响。\n",
    "\n",
    "### TPR & FPR\n",
    "\n",
    "$$TPR(True Postive Rate) = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "$$FPR(False Postive Rate) = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "### ROC & AUC\n",
    "\n",
    "[ROC from Scikit-learn:](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py)\n",
    "\n",
    "Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality.\n",
    "\n",
    "ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the “ideal” point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\n",
    "\n",
    "The “steepness” of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.\n",
    "\n",
    "Process for limited examples (my description):\n",
    "\n",
    "Prepares both m+ positive and m- negative samples and sort out them by predicted posibility in desceding order. Then set the highest classifier threshold  (marks all as negative) to make TPR and FPR on (0,0). Level down the threshold just enough to make samples output positive, one by one. Mark previous sample as (x,y). If current sample is true positve, marks $(x, y + \\frac{1}{m+})$ on ROC. Otherwise for false positive, marks $(x + \\frac{1}{m-}, y)$ instead.\n",
    "\n",
    "If one classifier's ROC totally cover one another, it should be better. If cross-over happens, AUC area should be counted on.  \n",
    "\n",
    "![image](http://scikit-learn.org/stable/_images/sphx_glr_plot_roc_001.png)\n",
    "\n",
    "![image](http://scikit-learn.org/stable/_images/sphx_glr_plot_roc_002.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
