{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "1. [聚类算法-知乎](https://www.zhihu.com/question/34554321)\n",
    "2. [Sciki-learn Clustering](http://scikit-learn.org/stable/modules/clustering.html#clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似性衡量（similarity measurement）\n",
    "\n",
    "### Manhattan distance (L1 norm)\n",
    "\n",
    "$$sim(X,Y) = |x_1 - x_2| + |y_1 - y_2|$$\n",
    "\n",
    "### Euclidean Metric (L2 norm)\n",
    "\n",
    "$$sim(X,Y)  = \\sqrt{|x_1 - x_2|^2 + |y_1 - y_2|^2}$$\n",
    "\n",
    "\n",
    "### Cosine Metric\n",
    "\n",
    "$$cos(\\theta) = \\frac{x_1 * x_2 + y_1 * y_2}{\\sqrt{x_1^2 + y_1^2}*\\sqrt{x_2^2 + y_2^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 聚类算法（clustering algorithm）\n",
    "- Hierarchical methods：<br>\n",
    "该主要有两种路径：agglomerative和divisive，也可以理解为自下而上法（bottom-up）和自上而下法（top-down）。自下而上法就是一开始每个个体（object）都是一个类，然后根据linkage寻找同类，最后形成一个“类”。自上而下法就是反过来，一开始所有个体都属于一个“类”，然后根据linkage排除异己，最后每个个体都成为一个“类”。这两种路径本质上没有孰优孰劣之分，只是在实际应用的时候要根据数据特点以及你想要的“类”的个数，来考虑是自上而下更快还是自下而上更快。至于根据Linkage判断“类”的方法就是楼上所提到的最短距离法、最长距离法、中间距离法、类平均法等等（其中类平均法往往被认为是最常用也最好用的方法，一方面因为其良好的单调性，另一方面因为其空间扩张/浓缩的程度适中）。Hierarchical methods中比较新的算法有BIRCH（Balanced Iterative Reducing and Clustering Using Hierarchies）主要是在数据体量很大的时候使用，而且数据类型是numerical；ROCK（A Hierarchical Clustering Algorithm for Categorical Attributes）主要用在categorical的数据类型上；Chameleon（A Hierarchical Clustering Algorithm Using Dynamic Modeling）里用到的linkage是kNN（k-nearest-neighbor）算法，并以此构建一个graph，Chameleon的聚类效果被认为非常强大，比BIRCH好用，但运算复杂的发很高，$O(n^2)$。看个Chameleon的聚类效果图，其中一个颜色代表一类，可以看出来是可以处理非常复杂的形状的。\n",
    "\n",
    "- Partition-based methods: <br>\n",
    "Partition-based methods：其原理简单来说就是，想象你有一堆散点需要聚类，想要的聚类效果就是“类内的点都足够近，类间的点都足够远”。首先你要确定这堆散点最后聚成几类，然后挑选几个点作为初始中心点，再然后依据预先定好的启发式算法（heuristic algorithms）给数据点做迭代重置（iterative relocation），直到最后到达“类内的点都足够近，类间的点都足够远”的目标效果。也正是根据所谓的“启发式算法”，形成了k-means算法及其变体包括k-medoids、k-modes、k-medians、kernel k-means等算法。k-means对初始值的设置很敏感，所以有了k-means++、intelligent k-means、genetic k-means；k-means对噪声和离群值非常敏感，所以有了k-medoids和k-medians；k-means只用于numerical类型数据，不适用于categorical类型数据，所以k-modes；k-means不能解决非凸（non-convex）数据，所以有了kernel k-means。另外，很多教程都告诉我们Partition-based methods聚类多适用于中等体量的数据集，但我们也不知道“中等”到底有多“中”，所以不妨理解成，数据集越大，越有可能陷入局部最小。下图显示的就是面对非凸，k-means和kernel k-means的不同效果。\n",
    "\n",
    "- Density-based methods: <br>\n",
    "上面这张图你也看到了，k-means解决不了这种不规则形状的聚类。于是就有了Density-based methods来系统解决这个问题。该方法同时也对噪声数据的处理比较好。其原理简单说画圈儿，其中要定义两个参数，一个是圈儿的最大半径，一个是一个圈儿里最少应容纳几个点。最后在一个圈里的，就是一个类。DBSCAN（Density-Based Spatial Clustering of Applications with Noise）就是其中的典型，可惜参数设置也是个问题，对这两个参数的设置非常敏感。DBSCAN的扩展叫OPTICS（Ordering Points To Identify Clustering Structure）通过优先对高密度（high density）进行搜索，然后根据高密度的特点设置参数，改善了DBSCAN的不足。下图就是表现了DBSCAN对参数设置的敏感，你们可以感受下。\n",
    "\n",
    "- Grid-based methods: <br>\n",
    "这类方法的原理就是将数据空间划分为网格单元，将数据对象集映射到网格单元中，并计算每个单元的密度。根据预设的阈值判断每个网格单元是否为高密度单元，由邻近的稠密单元组形成”类“。该类方法的优点就是执行效率高，因为其速度与数据对象的个数无关，而只依赖于数据空间中每个维上单元的个数。但缺点也是不少，比如对参数敏感、无法处理不规则分布的数据、维数灾难等。STING（STatistical INformation Grid）和CLIQUE（CLustering In QUEst）是该类方法中的代表性算法。下图是CLIQUE的一个例子：\n",
    "- Model-based methods：<br>\n",
    "这一类方法主要是指基于概率模型的方法和基于神经网络模型的方法，尤其以基于概率模型的方法居多。这里的概率模型主要指概率生成模型（generative Model），同一”类“的数据属于同一种概率分布。这中方法的优点就是对”类“的划分不那么”坚硬“，而是以概率形式表现，每一类的特征也可以用参数来表达；但缺点就是执行效率不高，特别是分布数量很多并且数据量很少的时候。其中最典型、也最常用的方法就是高斯混合模型（GMM，Gaussian Mixture Models）。基于神经网络模型的方法主要就是指SOM（Self Organized Maps）了，也是我所知的唯一一个非监督学习的神经网络了。下图表现的就是GMM的一个demo，里面用到EM算法来做最大似然估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "[scikit-learn 源码解读之Kmeans](http://midday.me/article/f8d29baa83ae41ec8c9826401eb7685e)\n",
    "\n",
    "已知观测集$(x_1,x_2,…,x_n)$，其中每个观测都是一个d-维实向量，k-平均聚类要把这n个观测划分到k个集合中(k ≤ n) $S = \\{S_1, S_2, …, S_k\\}$ ,使得组内平方和最小。换句话说，它的目标是找到使得满足下式的聚类 $S_{i}$，其中$\\mu _{i}$是 $S_{i}$中所有点的均值。\n",
    "\n",
    "$$arg\\min_{S}\\sum_{i=1}^k \\sum_{x\\in S} (||x - \\mu_i||^2)$$\n",
    "\n",
    "优点\n",
    "- 原理简单\n",
    "- 速度快\n",
    "- 对大数据集有比较好的伸缩性\n",
    "\n",
    "缺点\n",
    "- 需要指定聚类 数量K\n",
    "- 对异常值敏感\n",
    "- 对初始值敏感\n",
    "\n",
    "伪代码如下：\n",
    "\n",
    "1. 选取k个质心（这一步会影响整个聚类的结果）\n",
    "2. 将任意一个点分配到离质心最近的簇。\n",
    "3. 用每个簇的均值作为质心。\n",
    "4. 重复2和3直到所有点的分配结果不再发生变化，或者误差小于给定误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          9.91611221]\n",
      " [ 1.         12.44128511]\n",
      " [ 0.          9.91611221]\n",
      " [ 1.         12.44128511]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding=utf-8-*-\n",
    "import numpy as np\n",
    "def get_test_data():\n",
    "    \"\"\"\n",
    "    调试数据\n",
    "    \"\"\"\n",
    "    data = [\n",
    "       [ 1.658985, 4.285136],\n",
    "       [-3.453687, 3.424321],\n",
    "       [4.838138, -1.151539],\n",
    "       [-5.379713, -3.362104],\n",
    "    ]\n",
    "    return np.mat(data)\n",
    "def dist_eclud(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    计算距离\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.power(vec_a-vec_b, 2)))\n",
    "def rand_cent(data_set, k):\n",
    "    \"\"\"\n",
    "    随机选取质心\n",
    "    \"\"\"\n",
    "    m = np.shape(data_set)[1]\n",
    "    center = np.mat(np.zeros((k, m)))\n",
    "    for col in range(m):\n",
    "        min_col = min(data_set[:, col])\n",
    "        max_col = max(data_set[:, col])\n",
    "        center[:, col] = min_col + float((max_col-min_col)) * np.random.rand(k, 1)\n",
    "    return center\n",
    "def kmeans(data_set, k, dist_method=dist_eclud, cent_methon=rand_cent):\n",
    "    sample_count = np.shape(data_set)[0]\n",
    "    is_change = True\n",
    "    keep_result = np.mat(np.zeros((sample_count, 2)))\n",
    "    center_roids= cent_methon(data_set, k)\n",
    "    while is_change:\n",
    "        is_change = False\n",
    "        for sample_index in range(sample_count):\n",
    "            min_dist, min_index = np.Inf, -1\n",
    "            for j in range(k):\n",
    "                dist_j = dist_method(data_set[sample_index,:], center_roids[j,:])\n",
    "                if dist_j< min_dist:\n",
    "                    min_dist , min_index = dist_j , j\n",
    "            if keep_result[sample_index, 0] != min_index:\n",
    "                is_change = True\n",
    "            keep_result[sample_index,:] = min_index, min_dist**2\n",
    "        for cent_index in range(k):\n",
    "            temp_cluster = data_set[np.nonzero(keep_result[:,0].A==cent_index)[0]]\n",
    "            center_roids[cent_index,:] = np.mean(temp_cluster, axis=0)\n",
    "    return  keep_result\n",
    "if __name__ == '__main__':\n",
    "    data_set = get_test_data()\n",
    "    result = kmeans(data_set,2)\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "优点\n",
    "- 简单，易于理解，易于实现，无需估计参数，无需训练；\n",
    "- 适合对稀有事件进行分类；\n",
    "- 特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好。\n",
    "\n",
    "缺点\n",
    "- 该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 \n",
    "- 该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。\n",
    "- 该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。\n",
    "可理解性差，无法给出像决策树那样的规则。\n",
    "\n",
    "算法流程：\n",
    "\n",
    "1. 准备数据，对数据进行预处理\n",
    "2. 选用合适的数据结构存储训练数据和测试元组\n",
    "3. 设定参数，如k\n",
    "4. 维护一个大小为k的的按距离由大到小的优先级队列，用于存储最近邻训练元组。随机从训练元组中选取k个元组作为初始的最近邻元组，分别计算测试元组到这k个元组的距离，将训练元组标号和距离存入优先级队列\n",
    "5. 遍历训练元组集，计算当前训练元组与测试元组的距离，将所得距离L 与优先级队列中的最大距离Lmax\n",
    "6. 进行比较。若L>=Lmax，则舍弃该元组，遍历下一个元组。若L < Lmax，删除优先级队列中最大距离的元组，将当前训练元组存入优先级队列。\n",
    "7. 遍历完毕，计算优先级队列中k个元组的多数类，并将其作为测试元组的类别。\n",
    "8. 测试元组集测试完毕后计算误差率，继续设定不同的k值重新进行训练，最后取误差率最小的k 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 93\n",
      "Test set: 57\n",
      "Accuracy:  96.49122807017544\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sep 18 12:25:09 2014\n",
    "@author: mario\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "\n",
    "# Split the data into training and test data\n",
    "def loadDataset(filename, split, trainingSet=[] , testSet=[]):\n",
    "    with open(filename) as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "        for x in range(len(dataset)):\n",
    "            for y in range(4):\n",
    "                dataset[x][y] = float(dataset[x][y])\n",
    "            if random.random() < split:\n",
    "                trainingSet.append(dataset[x])\n",
    "            else:\n",
    "                testSet.append(dataset[x])\n",
    "                \n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    "    \n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "    distances.sort(key = operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    "    \n",
    "def getResponse(neighbors):\n",
    "    # Creating a list with all the possible neighbors\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "    \n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    "                \n",
    "def main():\n",
    "    trainingSet=[]\n",
    "    testSet=[]\n",
    "    split = 0.67\n",
    "    loadDataset('data/iris.data.csv', split, trainingSet, testSet)\n",
    "    print ('Train set: ' + repr(len(trainingSet)))\n",
    "    print ('Test set: ' + repr(len(testSet)))    \n",
    "    predictions=[]\n",
    "    k = 3\n",
    "    for x in range(len(testSet)):\n",
    "        neighbors = getNeighbors(trainingSet, testSet[x], k)\n",
    "        result = getResponse(neighbors)\n",
    "        predictions.append(result)\n",
    "        #print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))\n",
    "    accuracy = getAccuracy(testSet, predictions)\n",
    "    print ('Accuracy: ', accuracy)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。DBSCAN对用户定义的参数很敏感，细微的不同都可能导致差别很大的结果，而参数的选择无规律可循，只能靠经验确定。\n",
    "\n",
    "好处\n",
    "\n",
    "1. 与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。\n",
    "2. 与K-means方法相比，DBSCAN可以发现任意形状的簇类。\n",
    "3. 同时，DBSCAN能够识别出噪声点。\n",
    "4. DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。\n",
    "\n",
    "缺点\n",
    "\n",
    "1. DBScan不能很好反映高维数据。\n",
    "2. DBScan不能很好反映数据集以变化的密度。\n",
    "3. 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差。\n",
    "\n",
    "几个必要概念： \n",
    "\n",
    "- ε-邻域：对于样本集中的xj, 它的ε-邻域为样本集中与它距离小于ε的样本所构成的集合。\n",
    "\n",
    "- 核心对象：若xj的ε-邻域中至少包含MinPts个样本，则xj为一个核心对象。\n",
    "\n",
    "- 密度直达：若xj位于xi的ε-邻域中，且xi为核心对象，则xj由xi密度直达。\n",
    "\n",
    "- 密度可达：若样本序列p1, p2, ……, pn。pi+1由pi密度直达，则p1由pn密度可达。\n",
    "\n",
    "[DBSCAN算法描述:](https://blog.csdn.net/u014028027/article/details/72185796)\n",
    "\n",
    "输入: 包含n个对象的数据库，半径ε，最少数目MinPts;\n",
    "\n",
    "输出: 所有生成的簇，达到密度要求。\n",
    "\n",
    "1. 初始化核心对象集合T为空，遍历一遍样本集D中所有的样本，计算每个样本点的ε-邻域中包含样本的个数，如果个数大于等于MinPts，则将该样本点加入到核心对象集合T中。初始化聚类簇数k = 0， 初始化未访问样本集和为P = D。\n",
    "2. 当T集合中存在样本时执行如下步骤： \n",
    "    1. 记录当前未访问集合P_old = P \n",
    "    2. 从T中随机选一个核心对象o,初始化一个队列Q = [o] \n",
    "    3. P = P-o(从T中删除o) \n",
    "    4. 当Q中存在样本时执行： \n",
    "        1. 取出队列中的首个样本q \n",
    "        2. 计算q的ε-邻域中包含样本的个数，如果大于等于MinPts，则令S为q的ε-邻域与P的交集，Q = Q+S, P = P-S \n",
    "    5. k = k + 1, 生成聚类簇为Ck = P_old - P \n",
    "    6. T = T - Ck\n",
    "3. 划分为C= {C1, C2, ……, Ck}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " #-*- coding:utf-8 -*-\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    " #数据集：每三个是一组分别是西瓜的编号，密度，含糖量\n",
    "data = \"\"\"\n",
    "1,0.697,0.46,2,0.774,0.376,3,0.634,0.264,4,0.608,0.318,5,0.556,0.215,\n",
    "6,0.403,0.237,7,0.481,0.149,8,0.437,0.211,9,0.666,0.091,10,0.243,0.267,\n",
    "11,0.245,0.057,12,0.343,0.099,13,0.639,0.161,14,0.657,0.198,15,0.36,0.37,\n",
    "16,0.593,0.042,17,0.719,0.103,18,0.359,0.188,19,0.339,0.241,20,0.282,0.257,\n",
    "21,0.748,0.232,22,0.714,0.346,23,0.483,0.312,24,0.478,0.437,25,0.525,0.369,\n",
    "26,0.751,0.489,27,0.532,0.472,28,0.473,0.376,29,0.725,0.445,30,0.446,0.459\"\"\"\n",
    "\n",
    "#数据处理 dataset是30个样本（密度，含糖量）的列表\n",
    "a = data.split(',')\n",
    "dataset = [(float(a[i]), float(a[i+1])) for i in range(1, len(a)-1, 3)]\n",
    "\n",
    "#计算欧几里得距离,a,b分别为两个元组\n",
    "def dist(a, b):\n",
    "    return math.sqrt(math.pow(a[0]-b[0], 2)+math.pow(a[1]-b[1], 2))\n",
    "\n",
    "#算法模型\n",
    "def DBSCAN(D, e, Minpts):\n",
    "    #初始化核心对象集合T,聚类个数k,聚类集合C, 未访问集合P,\n",
    "    T = set(); k = 0; C = []; P = set(D)\n",
    "    for d in D:\n",
    "        if len([ i for i in D if dist(d, i) <= e]) >= Minpts:\n",
    "            T.add(d)\n",
    "    #开始聚类\n",
    "    while len(T):\n",
    "        P_old = P\n",
    "        o = list(T)[np.random.randint(0, len(T))]\n",
    "        P = P - set(o)\n",
    "        Q = []; Q.append(o)\n",
    "        while len(Q):\n",
    "            q = Q[0]\n",
    "            Nq = [i for i in D if dist(q, i) <= e]\n",
    "            if len(Nq) >= Minpts:\n",
    "                S = P & set(Nq)\n",
    "                Q += (list(S))\n",
    "                P = P - S\n",
    "            Q.remove(q)\n",
    "        k += 1\n",
    "        Ck = list(P_old - P)\n",
    "        T = T - set(Ck)\n",
    "        C.append(Ck)\n",
    "    return C\n",
    "\n",
    "#画图\n",
    "def draw(C):\n",
    "    colValue = ['r', 'y', 'g', 'b', 'c', 'k', 'm']\n",
    "    for i in range(len(C)):\n",
    "        coo_X = []    #x坐标列表\n",
    "        coo_Y = []    #y坐标列表\n",
    "        for j in range(len(C[i])):\n",
    "            coo_X.append(C[i][j][0])\n",
    "            coo_Y.append(C[i][j][1])\n",
    "        pl.scatter(coo_X, coo_Y, marker='x', color=colValue[i%len(colValue)], label=i)\n",
    "\n",
    "    pl.legend(loc='upper right')\n",
    "    pl.show()\n",
    "\n",
    "C = DBSCAN(dataset, 0.11, 5)\n",
    "draw(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯混合模型（Gaussian Mixed Model）\n",
    "\n",
    "[高斯混合模型（Gaussian Mixed Model）](https://blog.csdn.net/jinping_shi/article/details/59613054)指的是多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况（或者是同一类分布但参数不一样，或者是不同类型的分布，比如正态分布和伯努利分布）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据简化（data reduction)\n",
    "<ul>\n",
    "<li>变换（Data Transformation）：离散傅里叶变换（Discrete Fourier Transformation）可以提取数据的频域（frequency domain）信息，离散小波变换（Discrete Wavelet Transformation）除了频域之外，还可以提取到时域（temporal domain）信息。<br></li>\n",
    "<li>降维（Dimensionality Reduction）：在降维的方法中，PCA（Principle Component Analysis）和SVD（Singular Value Decomposition）作为线性方法，受到最广泛的应用。还有像MDS（Multi-Dimensional Scaling）什么的，不过只是作为PCA的一个扩展，给我的感觉是中看不中用。这几个方法局限肯定是无法处理非线性特征明显的数据。处理非线性降维的算法主要是流形学习（Manifold Learning），这又是一大块内容，里面集中常见的算法包括ISOMAP、LLE（Locally Linear Embedding）、MVU（Maximum variance unfolding）、Laplacian eigenmaps、Hessian eigenmaps、Kernel PCA、Probabilistic PCA等等。流形学习还是挺有趣的，而且一直在发展。关于降维在聚类中的应用，最著名的应该就是 @宋超 在评论里提到的谱聚类（Spectral Clustering），就是先用Laplacian eigenmaps对数据降维（简单地说，就是先将数据转换成邻接矩阵或相似性矩阵，再转换成Laplacian矩阵，再对Laplacian矩阵进行特征分解，把最小的K个特征向量排列在一起），然后再使用k-means完成聚类。谱聚类是个很好的方法，效果通常比k-means好，计算复杂度还低，这都要归功于降维的作用。</li>\n",
    "<li>抽样（Sampling）：最常用的就是随机抽样（Random Sampling）咯，如果你的数据集特别大，随机抽样就越能显示出它的低复杂性所带来的好处。比如CLARA（Clustering LARge Applications）就是因为k-medoids应对不了大规模的数据集，所以采用sampling的方法。至于更加fancy的抽样方法我还真不了解，我就不在这里好为人师而误人子弟了。</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
