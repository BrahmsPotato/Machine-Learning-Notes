{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "1. [聚类算法-知乎](https://www.zhihu.com/question/34554321)\n",
    "2. [Sciki-learn Clustering](http://scikit-learn.org/stable/modules/clustering.html#clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似性衡量（similarity measurement）\n",
    "\n",
    "### Manhattan distance (L1 norm)\n",
    "\n",
    "$$sim(X,Y) = |x_1 - x_2| + |y_1 - y_2|$$\n",
    "\n",
    "### Euclidean Metric (L2 norm)\n",
    "\n",
    "$$sim(X,Y)  = \\sqrt{|x_1 - x_2|^2 + |y_1 - y_2|^2}$$\n",
    "\n",
    "\n",
    "### Cosine Metric\n",
    "\n",
    "$$cos(\\theta) = \\frac{x_1 * x_2 + y_1 * y_2}{\\sqrt{x_1^2 + y_1^2}\\sqrt{x_2^2 + y_2^2}} $$\n",
    "\n",
    "### Pearson correlation coefficient\n",
    "\n",
    "$$ r = \\frac{\\sum_{i=1}^n (x_i - \\overline x)(y_i - \\overline y)}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline x)^2} \\sqrt{\\sum_{i=1}^n (y_i - \\overline y)^2} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 协同过滤 (Collaborative Filtering)\n",
    "\n",
    "[深入推荐引擎相关算法 - 协同过滤](https://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/index.html?ca=drs-)\n",
    "\n",
    "### 基于用户的 CF（User CF）\n",
    "\n",
    "基于用户的 CF 的基本思想相当简单，基于用户对物品的偏好找到相邻邻居用户，然后将邻居用户喜欢的推荐给当前用户。计算上，就是将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，找到 K 邻居后，根据邻居的相似度权重以及他们对物品的偏好，预测当前用户没有偏好的未涉及物品，计算得到一个排序的物品列表作为推荐。图 2 给出了一个例子，对于用户 A，根据用户的历史偏好，这里只计算得到一个邻居 - 用户 C，然后将用户 C 喜欢的物品 D 推荐给用户 A。\n",
    "\n",
    "<img src=\"images/cf_user.gif\" style=\"width: 400px;\"/>\n",
    "\n",
    "### 基于物品的 CF（Item CF）\n",
    "\n",
    "基于物品的 CF 的原理和基于用户的 CF 类似，只是在计算邻居时采用物品本身，而不是从用户的角度，即基于用户对物品的偏好找到相似的物品，然后根据用户的历史偏好，推荐相似的物品给他。从计算的角度看，就是将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度，得到物品的相似物品后，根据用户历史的偏好预测当前用户还没有表示偏好的物品，计算得到一个排序的物品列表作为推荐。图 3 给出了一个例子，对于物品 A，根据所有用户的历史偏好，喜欢物品 A 的用户都喜欢物品 C，得出物品 A 和物品 C 比较相似，而用户 C 喜欢物品 A，那么可以推断出用户 C 可能也喜欢物品 C。\n",
    "\n",
    "<img src=\"images/cf_item.gif\" style=\"width: 400px;\"/>\n",
    "\n",
    "### User CF vs. Item CF\n",
    "\n",
    "前面介绍了 User CF 和 Item CF 的基本原理，下面我们分几个不同的角度深入看看它们各自的优缺点和适用场景：\n",
    "\n",
    "#### 计算复杂度\n",
    "Item CF 和 User CF 是基于协同过滤推荐的两个最基本的算法，User CF 是很早以前就提出来了，Item CF 是从 Amazon 的论文和专利发表之后（2001 年左右）开始流行，大家都觉得 Item CF 从性能和复杂度上比 User CF 更优，其中的一个主要原因就是对于一个在线网站，用户的数量往往大大超过物品的数量，同时物品的数据相对稳定，因此计算物品的相似度不但计算量较小，同时也不必频繁更新。但我们往往忽略了这种情况只适应于提供商品的电子商务网站，对于新闻，博客或者微内容的推荐系统，情况往往是相反的，物品的数量是海量的，同时也是更新频繁的，所以单从复杂度的角度，这两个算法在不同的系统中各有优势，推荐引擎的设计者需要根据自己应用的特点选择更加合适的算法。\n",
    "\n",
    "#### 适用场景\n",
    "在非社交网络的网站中，内容内在的联系是很重要的推荐原则，它比基于相似用户的推荐原则更加有效。比如在购书网站上，当你看一本书的时候，推荐引擎会给你推荐相关的书籍，这个推荐的重要性远远超过了网站首页对该用户的综合推荐。可以看到，在这种情况下，Item CF 的推荐成为了引导用户浏览的重要手段。同时 Item CF 便于为推荐做出解释，在一个非社交网络的网站中，给某个用户推荐一本书，同时给出的解释是某某和你有相似兴趣的人也看了这本书，这很难让用户信服，因为用户可能根本不认识那个人；但如果解释说是因为这本书和你以前看的某本书相似，用户可能就觉得合理而采纳了此推荐。\n",
    "\n",
    "相反的，在现今很流行的社交网络站点中，User CF 是一个更不错的选择，User CF 加上社会网络信息，可以增加用户对推荐解释的信服程度。\n",
    "\n",
    "#### 推荐多样性和精度\n",
    "研究推荐引擎的学者们在相同的数据集合上分别用 User CF 和 Item CF 计算推荐结果，发现推荐列表中，只有 50% 是一样的，还有 50% 完全不同。但是这两个算法确有相似的精度，所以可以说，这两个算法是很互补的。\n",
    "\n",
    "关于推荐的多样性，有两种度量方法：\n",
    "\n",
    "第一种度量方法是从单个用户的角度度量，就是说给定一个用户，查看系统给出的推荐列表是否多样，也就是要比较推荐列表中的物品之间两两的相似度，不难想到，对这种度量方法，Item CF 的多样性显然不如 User CF 的好，因为 Item CF 的推荐就是和以前看的东西最相似的。\n",
    "\n",
    "第二种度量方法是考虑系统的多样性，也被称为覆盖率 (Coverage)，它是指一个推荐系统是否能够提供给所有用户丰富的选择。在这种指标下，Item CF 的多样性要远远好于 User CF, 因为 User CF 总是倾向于推荐热门的，从另一个侧面看，也就是说，Item CF 的推荐有很好的新颖性，很擅长推荐长尾里的物品。所以，尽管大多数情况，Item CF 的精度略小于 User CF， 但如果考虑多样性，Item CF 却比 User CF 好很多。\n",
    "\n",
    "如果你对推荐的多样性还心存疑惑，那么下面我们再举个实例看看 User CF 和 Item CF 的多样性到底有什么差别。首先，假设每个用户兴趣爱好都是广泛的，喜欢好几个领域的东西，不过每个用户肯定也有一个主要的领域，对这个领域会比其他领域更加关心。给定一个用户，假设他喜欢 3 个领域 A,B,C，A 是他喜欢的主要领域，这个时候我们来看 User CF 和 Item CF 倾向于做出什么推荐：如果用 User CF, 它会将 A,B,C 三个领域中比较热门的东西推荐给用户；而如果用 ItemCF，它会基本上只推荐 A 领域的东西给用户。所以我们看到因为 User CF 只推荐热门的，所以它在推荐长尾里项目方面的能力不足；而 Item CF 只推荐 A 领域给用户，这样他有限的推荐列表中就可能包含了一定数量的不热门的长尾物品，同时 Item CF 的推荐对这个用户而言，显然多样性不足。但是对整个系统而言，因为不同的用户的主要兴趣点不同，所以系统的覆盖率会比较好。\n",
    "\n",
    "从上面的分析，可以很清晰的看到，这两种推荐都有其合理性，但都不是最好的选择，因此他们的精度也会有损失。其实对这类系统的最好选择是，如果系统给这个用户推荐 30 个物品，既不是每个领域挑选 10 个最热门的给他，也不是推荐 30 个 A 领域的给他，而是比如推荐 15 个 A 领域的给他，剩下的 15 个从 B,C 中选择。所以结合 User CF 和 Item CF 是最优的选择，结合的基本原则就是当采用 Item CF 导致系统对个人推荐的多样性不足时，我们通过加入 User CF 增加个人推荐的多样性，从而提高精度，而当因为采用 User CF 而使系统的整体多样性不足时，我们可以通过加入 Item CF 增加整体的多样性，同样同样可以提高推荐的精度。\n",
    "\n",
    "#### 用户对推荐算法的适应度\n",
    "前面我们大部分都是从推荐引擎的角度考虑哪个算法更优，但其实我们更多的应该考虑作为推荐引擎的最终使用者 -- 应用用户对推荐算法的适应度。\n",
    "\n",
    "对于 User CF，推荐的原则是假设用户会喜欢那些和他有相同喜好的用户喜欢的东西，但如果一个用户没有相同喜好的朋友，那 User CF 的算法的效果就会很差，所以一个用户对的 CF 算法的适应度是和他有多少共同喜好用户成正比的。\n",
    "\n",
    "Item CF 算法也有一个基本假设，就是用户会喜欢和他以前喜欢的东西相似的东西，那么我们可以计算一个用户喜欢的物品的自相似度。一个用户喜欢物品的自相似度大，就说明他喜欢的东西都是比较相似的，也就是说他比较符合 Item CF 方法的基本假设，那么他对 Item CF 的适应度自然比较好；反之，如果自相似度小，就说明这个用户的喜好习惯并不满足 Item CF 方法的基本假设，那么对于这种用户，用 Item CF 方法做出好的推荐的可能性非常低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 聚类算法（clustering algorithm）\n",
    "- Hierarchical methods：<br>\n",
    "该主要有两种路径：agglomerative和divisive，也可以理解为自下而上法（bottom-up）和自上而下法（top-down）。自下而上法就是一开始每个个体（object）都是一个类，然后根据linkage寻找同类，最后形成一个“类”。自上而下法就是反过来，一开始所有个体都属于一个“类”，然后根据linkage排除异己，最后每个个体都成为一个“类”。这两种路径本质上没有孰优孰劣之分，只是在实际应用的时候要根据数据特点以及你想要的“类”的个数，来考虑是自上而下更快还是自下而上更快。至于根据Linkage判断“类”的方法就是楼上所提到的最短距离法、最长距离法、中间距离法、类平均法等等（其中类平均法往往被认为是最常用也最好用的方法，一方面因为其良好的单调性，另一方面因为其空间扩张/浓缩的程度适中）。Hierarchical methods中比较新的算法有BIRCH（Balanced Iterative Reducing and Clustering Using Hierarchies）主要是在数据体量很大的时候使用，而且数据类型是numerical；ROCK（A Hierarchical Clustering Algorithm for Categorical Attributes）主要用在categorical的数据类型上；Chameleon（A Hierarchical Clustering Algorithm Using Dynamic Modeling）里用到的linkage是kNN（k-nearest-neighbor）算法，并以此构建一个graph，Chameleon的聚类效果被认为非常强大，比BIRCH好用，但运算复杂的发很高，$O(n^2)$。看个Chameleon的聚类效果图，其中一个颜色代表一类，可以看出来是可以处理非常复杂的形状的。\n",
    "\n",
    "- Partition-based methods: <br>\n",
    "Partition-based methods：其原理简单来说就是，想象你有一堆散点需要聚类，想要的聚类效果就是“类内的点都足够近，类间的点都足够远”。首先你要确定这堆散点最后聚成几类，然后挑选几个点作为初始中心点，再然后依据预先定好的启发式算法（heuristic algorithms）给数据点做迭代重置（iterative relocation），直到最后到达“类内的点都足够近，类间的点都足够远”的目标效果。也正是根据所谓的“启发式算法”，形成了k-means算法及其变体包括k-medoids、k-modes、k-medians、kernel k-means等算法。k-means对初始值的设置很敏感，所以有了k-means++、intelligent k-means、genetic k-means；k-means对噪声和离群值非常敏感，所以有了k-medoids和k-medians；k-means只用于numerical类型数据，不适用于categorical类型数据，所以k-modes；k-means不能解决非凸（non-convex）数据，所以有了kernel k-means。另外，很多教程都告诉我们Partition-based methods聚类多适用于中等体量的数据集，但我们也不知道“中等”到底有多“中”，所以不妨理解成，数据集越大，越有可能陷入局部最小。下图显示的就是面对非凸，k-means和kernel k-means的不同效果。\n",
    "\n",
    "- Density-based methods: <br>\n",
    "上面这张图你也看到了，k-means解决不了这种不规则形状的聚类。于是就有了Density-based methods来系统解决这个问题。该方法同时也对噪声数据的处理比较好。其原理简单说画圈儿，其中要定义两个参数，一个是圈儿的最大半径，一个是一个圈儿里最少应容纳几个点。最后在一个圈里的，就是一个类。DBSCAN（Density-Based Spatial Clustering of Applications with Noise）就是其中的典型，可惜参数设置也是个问题，对这两个参数的设置非常敏感。DBSCAN的扩展叫OPTICS（Ordering Points To Identify Clustering Structure）通过优先对高密度（high density）进行搜索，然后根据高密度的特点设置参数，改善了DBSCAN的不足。下图就是表现了DBSCAN对参数设置的敏感，你们可以感受下。\n",
    "\n",
    "- Grid-based methods: <br>\n",
    "这类方法的原理就是将数据空间划分为网格单元，将数据对象集映射到网格单元中，并计算每个单元的密度。根据预设的阈值判断每个网格单元是否为高密度单元，由邻近的稠密单元组形成”类“。该类方法的优点就是执行效率高，因为其速度与数据对象的个数无关，而只依赖于数据空间中每个维上单元的个数。但缺点也是不少，比如对参数敏感、无法处理不规则分布的数据、维数灾难等。STING（STatistical INformation Grid）和CLIQUE（CLustering In QUEst）是该类方法中的代表性算法。下图是CLIQUE的一个例子：\n",
    "- Model-based methods：<br>\n",
    "这一类方法主要是指基于概率模型的方法和基于神经网络模型的方法，尤其以基于概率模型的方法居多。这里的概率模型主要指概率生成模型（generative Model），同一”类“的数据属于同一种概率分布。这中方法的优点就是对”类“的划分不那么”坚硬“，而是以概率形式表现，每一类的特征也可以用参数来表达；但缺点就是执行效率不高，特别是分布数量很多并且数据量很少的时候。其中最典型、也最常用的方法就是高斯混合模型（GMM，Gaussian Mixture Models）。基于神经网络模型的方法主要就是指SOM（Self Organized Maps）了，也是我所知的唯一一个非监督学习的神经网络了。下图表现的就是GMM的一个demo，里面用到EM算法来做最大似然估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canopy\n",
    "\n",
    "Canopy 聚类算法的基本原则：\n",
    "\n",
    "首先应用成本低的近似的距离计算方法高效的将数据分为多个组，这里称为一个 Canopy，我们姑且将它翻译为“华盖”，Canopy 之间可以有重叠的部分；然后采用严格的距离计算方式准确的计算在同一 Canopy 中的点，将他们分配与最合适的簇中。Canopy 聚类算法经常用于 K 均值聚类算法的预处理，用来找合适的 k 值和簇中心。\n",
    "\n",
    "<img src=\"images/Canopy.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Canopy算法的步骤：\n",
    "\n",
    "1. 原始数据集合List按照一定的规则进行排序（这个规则是任意的，但是一旦确定就不再更改），初始距离阈值为T1、T2，且T1 ＞ T2（T1、T2的设定可以根据用户的需要，或者使用交叉验证获得）。\n",
    "\n",
    "2. 在List中随机挑选一个数据向量A，使用一个粗糙距离计算方式计算A与List中其他样本数据向量之间的距离d。\n",
    "\n",
    "3. 根据第2步中的距离d，把d小于T1的样本数据向量划到一个canopy中，同时把d小于T2的样本数据向量从候选中心向量名单（这里可以理解为就是List）中移除。\n",
    "\n",
    "4. 重复第2、3步，直到候选中心向量名单为空，即List为空，算法结束。\n",
    "\n",
    "\n",
    "Canopy算法的优缺点：\n",
    "\n",
    "1. 不需要事先指定k值（即clustering的个数）\n",
    "\n",
    "2. 精度较低，但其速度上有很大的优势\n",
    "\n",
    "3. 前期可以使用Canopy聚类先对数据进行“粗”聚类，得到k值后再使用K-means进行进一步的“细”聚类\n",
    "\n",
    "\n",
    "\n",
    "### K-means\n",
    "\n",
    "[scikit-learn 源码解读之Kmeans](http://midday.me/article/f8d29baa83ae41ec8c9826401eb7685e)\n",
    "\n",
    "已知观测集$(x_1,x_2,…,x_n)$，其中每个观测都是一个d-维实向量，k-平均聚类要把这n个观测划分到k个集合中(k ≤ n) $S = \\{S_1, S_2, …, S_k\\}$ ,使得组内平方和最小。换句话说，它的目标是找到使得满足下式的聚类 $S_{i}$，其中$\\mu _{i}$是 $S_{i}$中所有点的均值。\n",
    "\n",
    "$$arg\\min_{S}\\sum_{i=1}^k \\sum_{x\\in S} (||x - \\mu_i||^2)$$\n",
    "\n",
    "优点\n",
    "- 原理简单\n",
    "- 速度快\n",
    "- 对大数据集有比较好的伸缩性\n",
    "\n",
    "缺点\n",
    "- 需要指定聚类 数量K\n",
    "- 对异常值敏感\n",
    "- 对初始值敏感\n",
    "\n",
    "伪代码如下：\n",
    "\n",
    "1. 选取k个质心（这一步会影响整个聚类的结果）\n",
    "2. 将任意一个点分配到离质心最近的簇。\n",
    "3. 用每个簇的均值作为质心。\n",
    "4. 重复2和3直到所有点的分配结果不再发生变化，或者误差小于给定误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          9.91611221]\n",
      " [ 1.         12.44128511]\n",
      " [ 0.          9.91611221]\n",
      " [ 1.         12.44128511]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding=utf-8-*-\n",
    "import numpy as np\n",
    "def get_test_data():\n",
    "    \"\"\"\n",
    "    调试数据\n",
    "    \"\"\"\n",
    "    data = [\n",
    "       [ 1.658985, 4.285136],\n",
    "       [-3.453687, 3.424321],\n",
    "       [4.838138, -1.151539],\n",
    "       [-5.379713, -3.362104],\n",
    "    ]\n",
    "    return np.mat(data)\n",
    "def dist_eclud(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    计算距离\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.power(vec_a-vec_b, 2)))\n",
    "def rand_cent(data_set, k):\n",
    "    \"\"\"\n",
    "    随机选取质心\n",
    "    \"\"\"\n",
    "    m = np.shape(data_set)[1]\n",
    "    center = np.mat(np.zeros((k, m)))\n",
    "    for col in range(m):\n",
    "        min_col = min(data_set[:, col])\n",
    "        max_col = max(data_set[:, col])\n",
    "        center[:, col] = min_col + float((max_col-min_col)) * np.random.rand(k, 1)\n",
    "    return center\n",
    "def kmeans(data_set, k, dist_method=dist_eclud, cent_methon=rand_cent):\n",
    "    sample_count = np.shape(data_set)[0]\n",
    "    is_change = True\n",
    "    keep_result = np.mat(np.zeros((sample_count, 2)))\n",
    "    center_roids= cent_methon(data_set, k)\n",
    "    while is_change:\n",
    "        is_change = False\n",
    "        for sample_index in range(sample_count):\n",
    "            min_dist, min_index = np.Inf, -1\n",
    "            for j in range(k):\n",
    "                dist_j = dist_method(data_set[sample_index,:], center_roids[j,:])\n",
    "                if dist_j< min_dist:\n",
    "                    min_dist , min_index = dist_j , j\n",
    "            if keep_result[sample_index, 0] != min_index:\n",
    "                is_change = True\n",
    "            keep_result[sample_index,:] = min_index, min_dist**2\n",
    "        for cent_index in range(k):\n",
    "            temp_cluster = data_set[np.nonzero(keep_result[:,0].A==cent_index)[0]]\n",
    "            center_roids[cent_index,:] = np.mean(temp_cluster, axis=0)\n",
    "    return  keep_result\n",
    "if __name__ == '__main__':\n",
    "    data_set = get_test_data()\n",
    "    result = kmeans(data_set,2)\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "优点\n",
    "- 简单，易于理解，易于实现，无需估计参数，无需训练；\n",
    "- 适合对稀有事件进行分类；\n",
    "- 特别适合于多分类问题(multi-modal,对象具有多个类别标签)， kNN比SVM的表现要好。\n",
    "\n",
    "缺点\n",
    "- 该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 \n",
    "- 该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。\n",
    "- 该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。\n",
    "可理解性差，无法给出像决策树那样的规则。\n",
    "\n",
    "算法流程：\n",
    "\n",
    "1. 准备数据，对数据进行预处理\n",
    "2. 选用合适的数据结构存储训练数据和测试元组\n",
    "3. 设定参数，如k\n",
    "4. 维护一个大小为k的的按距离由大到小的优先级队列，用于存储最近邻训练元组。随机从训练元组中选取k个元组作为初始的最近邻元组，分别计算测试元组到这k个元组的距离，将训练元组标号和距离存入优先级队列\n",
    "5. 遍历训练元组集，计算当前训练元组与测试元组的距离，将所得距离L 与优先级队列中的最大距离Lmax\n",
    "6. 进行比较。若L>=Lmax，则舍弃该元组，遍历下一个元组。若L < Lmax，删除优先级队列中最大距离的元组，将当前训练元组存入优先级队列。\n",
    "7. 遍历完毕，计算优先级队列中k个元组的多数类，并将其作为测试元组的类别。\n",
    "8. 测试元组集测试完毕后计算误差率，继续设定不同的k值重新进行训练，最后取误差率最小的k 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 93\n",
      "Test set: 57\n",
      "Accuracy:  96.49122807017544\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sep 18 12:25:09 2014\n",
    "@author: mario\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "\n",
    "# Split the data into training and test data\n",
    "def loadDataset(filename, split, trainingSet=[] , testSet=[]):\n",
    "    with open(filename) as csvfile:\n",
    "        lines = csv.reader(csvfile)\n",
    "        dataset = list(lines)\n",
    "        for x in range(len(dataset)):\n",
    "            for y in range(4):\n",
    "                dataset[x][y] = float(dataset[x][y])\n",
    "            if random.random() < split:\n",
    "                trainingSet.append(dataset[x])\n",
    "            else:\n",
    "                testSet.append(dataset[x])\n",
    "                \n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    "    \n",
    "def getNeighbors(trainingSet, testInstance, k):\n",
    "    distances = []\n",
    "    length = len(testInstance)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist = euclideanDistance(testInstance, trainingSet[x], length)\n",
    "        distances.append((trainingSet[x], dist))\n",
    "    distances.sort(key = operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(distances[x][0])\n",
    "    return neighbors\n",
    "    \n",
    "def getResponse(neighbors):\n",
    "    # Creating a list with all the possible neighbors\n",
    "    classVotes = {}\n",
    "    for x in range(len(neighbors)):\n",
    "        response = neighbors[x][-1]\n",
    "        if response in classVotes:\n",
    "            classVotes[response] += 1\n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "    \n",
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0\n",
    "                \n",
    "def main():\n",
    "    trainingSet=[]\n",
    "    testSet=[]\n",
    "    split = 0.67\n",
    "    loadDataset('data/iris.data.csv', split, trainingSet, testSet)\n",
    "    print ('Train set: ' + repr(len(trainingSet)))\n",
    "    print ('Test set: ' + repr(len(testSet)))    \n",
    "    predictions=[]\n",
    "    k = 3\n",
    "    for x in range(len(testSet)):\n",
    "        neighbors = getNeighbors(trainingSet, testSet[x], k)\n",
    "        result = getResponse(neighbors)\n",
    "        predictions.append(result)\n",
    "        #print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))\n",
    "    accuracy = getAccuracy(testSet, predictions)\n",
    "    print ('Accuracy: ', accuracy)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 是一个比较有代表性的基于密度的聚类算法。与划分和层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。DBSCAN对用户定义的参数很敏感，细微的不同都可能导致差别很大的结果，而参数的选择无规律可循，只能靠经验确定。\n",
    "\n",
    "好处\n",
    "\n",
    "1. 与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。\n",
    "2. 与K-means方法相比，DBSCAN可以发现任意形状的簇类。\n",
    "3. 同时，DBSCAN能够识别出噪声点。\n",
    "4. DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。\n",
    "\n",
    "缺点\n",
    "\n",
    "1. DBScan不能很好反映高维数据。\n",
    "2. DBScan不能很好反映数据集以变化的密度。\n",
    "3. 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差。\n",
    "\n",
    "几个必要概念： \n",
    "\n",
    "- ε-邻域：对于样本集中的xj, 它的ε-邻域为样本集中与它距离小于ε的样本所构成的集合。\n",
    "\n",
    "- 核心对象：若xj的ε-邻域中至少包含MinPts个样本，则xj为一个核心对象。\n",
    "\n",
    "- 密度直达：若xj位于xi的ε-邻域中，且xi为核心对象，则xj由xi密度直达。\n",
    "\n",
    "- 密度可达：若样本序列p1, p2, ……, pn。pi+1由pi密度直达，则p1由pn密度可达。\n",
    "\n",
    "[DBSCAN算法描述:](https://blog.csdn.net/u014028027/article/details/72185796)\n",
    "\n",
    "输入: 包含n个对象的数据库，半径ε，最少数目MinPts;\n",
    "\n",
    "输出: 所有生成的簇，达到密度要求。\n",
    "\n",
    "1. 初始化核心对象集合T为空，遍历一遍样本集D中所有的样本，计算每个样本点的ε-邻域中包含样本的个数，如果个数大于等于MinPts，则将该样本点加入到核心对象集合T中。初始化聚类簇数k = 0， 初始化未访问样本集和为P = D。\n",
    "2. 当T集合中存在样本时执行如下步骤： \n",
    "    1. 记录当前未访问集合P_old = P \n",
    "    2. 从T中随机选一个核心对象o,初始化一个队列Q = [o] \n",
    "    3. P = P-o(从T中删除o) \n",
    "    4. 当Q中存在样本时执行： \n",
    "        1. 取出队列中的首个样本q \n",
    "        2. 计算q的ε-邻域中包含样本的个数，如果大于等于MinPts，则令S为q的ε-邻域与P的交集，Q = Q+S, P = P-S \n",
    "    5. k = k + 1, 生成聚类簇为Ck = P_old - P \n",
    "    6. T = T - Ck\n",
    "3. 划分为C= {C1, C2, ……, Ck}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " #-*- coding:utf-8 -*-\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    " #数据集：每三个是一组分别是西瓜的编号，密度，含糖量\n",
    "data = \"\"\"\n",
    "1,0.697,0.46,2,0.774,0.376,3,0.634,0.264,4,0.608,0.318,5,0.556,0.215,\n",
    "6,0.403,0.237,7,0.481,0.149,8,0.437,0.211,9,0.666,0.091,10,0.243,0.267,\n",
    "11,0.245,0.057,12,0.343,0.099,13,0.639,0.161,14,0.657,0.198,15,0.36,0.37,\n",
    "16,0.593,0.042,17,0.719,0.103,18,0.359,0.188,19,0.339,0.241,20,0.282,0.257,\n",
    "21,0.748,0.232,22,0.714,0.346,23,0.483,0.312,24,0.478,0.437,25,0.525,0.369,\n",
    "26,0.751,0.489,27,0.532,0.472,28,0.473,0.376,29,0.725,0.445,30,0.446,0.459\"\"\"\n",
    "\n",
    "#数据处理 dataset是30个样本（密度，含糖量）的列表\n",
    "a = data.split(',')\n",
    "dataset = [(float(a[i]), float(a[i+1])) for i in range(1, len(a)-1, 3)]\n",
    "\n",
    "#计算欧几里得距离,a,b分别为两个元组\n",
    "def dist(a, b):\n",
    "    return math.sqrt(math.pow(a[0]-b[0], 2)+math.pow(a[1]-b[1], 2))\n",
    "\n",
    "#算法模型\n",
    "def DBSCAN(D, e, Minpts):\n",
    "    #初始化核心对象集合T,聚类个数k,聚类集合C, 未访问集合P,\n",
    "    T = set(); k = 0; C = []; P = set(D)\n",
    "    for d in D:\n",
    "        if len([ i for i in D if dist(d, i) <= e]) >= Minpts:\n",
    "            T.add(d)\n",
    "    #开始聚类\n",
    "    while len(T):\n",
    "        P_old = P\n",
    "        o = list(T)[np.random.randint(0, len(T))]\n",
    "        P = P - set(o)\n",
    "        Q = []; Q.append(o)\n",
    "        while len(Q):\n",
    "            q = Q[0]\n",
    "            Nq = [i for i in D if dist(q, i) <= e]\n",
    "            if len(Nq) >= Minpts:\n",
    "                S = P & set(Nq)\n",
    "                Q += (list(S))\n",
    "                P = P - S\n",
    "            Q.remove(q)\n",
    "        k += 1\n",
    "        Ck = list(P_old - P)\n",
    "        T = T - set(Ck)\n",
    "        C.append(Ck)\n",
    "    return C\n",
    "\n",
    "#画图\n",
    "def draw(C):\n",
    "    colValue = ['r', 'y', 'g', 'b', 'c', 'k', 'm']\n",
    "    for i in range(len(C)):\n",
    "        coo_X = []    #x坐标列表\n",
    "        coo_Y = []    #y坐标列表\n",
    "        for j in range(len(C[i])):\n",
    "            coo_X.append(C[i][j][0])\n",
    "            coo_Y.append(C[i][j][1])\n",
    "        pl.scatter(coo_X, coo_Y, marker='x', color=colValue[i%len(colValue)], label=i)\n",
    "\n",
    "    pl.legend(loc='upper right')\n",
    "    pl.show()\n",
    "\n",
    "C = DBSCAN(dataset, 0.11, 5)\n",
    "draw(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯混合模型（Gaussian Mixed Model）\n",
    "\n",
    "[高斯混合模型（Gaussian Mixed Model）](https://blog.csdn.net/jinping_shi/article/details/59613054)指的是多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况（或者是同一类分布但参数不一样，或者是不同类型的分布，比如正态分布和伯努利分布）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据简化（data reduction)\n",
    "<ul>\n",
    "<li>变换（Data Transformation）：离散傅里叶变换（Discrete Fourier Transformation）可以提取数据的频域（frequency domain）信息，离散小波变换（Discrete Wavelet Transformation）除了频域之外，还可以提取到时域（temporal domain）信息。<br></li>\n",
    "<li>降维（Dimensionality Reduction）：在降维的方法中，PCA（Principle Component Analysis）和SVD（Singular Value Decomposition）作为线性方法，受到最广泛的应用。还有像MDS（Multi-Dimensional Scaling）什么的，不过只是作为PCA的一个扩展，给我的感觉是中看不中用。这几个方法局限肯定是无法处理非线性特征明显的数据。处理非线性降维的算法主要是流形学习（Manifold Learning），这又是一大块内容，里面集中常见的算法包括ISOMAP、LLE（Locally Linear Embedding）、MVU（Maximum variance unfolding）、Laplacian eigenmaps、Hessian eigenmaps、Kernel PCA、Probabilistic PCA等等。流形学习还是挺有趣的，而且一直在发展。关于降维在聚类中的应用，最著名的应该就是 @宋超 在评论里提到的谱聚类（Spectral Clustering），就是先用Laplacian eigenmaps对数据降维（简单地说，就是先将数据转换成邻接矩阵或相似性矩阵，再转换成Laplacian矩阵，再对Laplacian矩阵进行特征分解，把最小的K个特征向量排列在一起），然后再使用k-means完成聚类。谱聚类是个很好的方法，效果通常比k-means好，计算复杂度还低，这都要归功于降维的作用。</li>\n",
    "<li>抽样（Sampling）：最常用的就是随机抽样（Random Sampling）咯，如果你的数据集特别大，随机抽样就越能显示出它的低复杂性所带来的好处。比如CLARA（Clustering LARge Applications）就是因为k-medoids应对不了大规模的数据集，所以采用sampling的方法。至于更加fancy的抽样方法我还真不了解，我就不在这里好为人师而误人子弟了。</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
