{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "[Activation Functions: Neural Networks](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "\n",
    "### Softmax\n",
    "\n",
    "$$softmax(z) = softmax(z +c) \\qquad  \\text{(c is a constant)}$$\n",
    "\n",
    "$$softmax(z)_i = \\frac{e^{z_i}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "$$L(z_i) = -log(softmax(z)_i)$$\n",
    "\n",
    "$$\\frac{\\partial{L(z_i)}}{\\partial{z_i}}= \\hat{y_{z_i}}-1 \\tag{1}$$\n",
    "\n",
    "> In (1), $ \\hat{y_{z_i}} $ is the softmax loss output for true class $z_i$. <br>\n",
    "e.g. $L(z) = [0.015,0.866,0.117 ].$ <br>\n",
    "if $\\hat{y_{z_2}} = 0.866$ is true ouput, $ \\partial{L_{z_2}}=0.866-1=-0.134, \\ \\frac{\\partial{L(z)}}{\\partial{z}} = [0.015,-0.134,0.117]$\n",
    "\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "<img src=\"images/sigmoid.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "$$sigmoid(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The main reason why we use sigmoid function is because it exists between **(0 to 1)**. Therefore, it is especially used for models where we have to predict the probability **as an output**.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "The function is **differentiable**.That means, we can find the slope of the sigmoid curve at any two points.\n",
    "\n",
    "The function is **monotonic** but functionâ€™s derivative is not.\n",
    "\n",
    "The logistic sigmoid function can cause a neural network to **get stuck** at the training time.\n",
    "\n",
    "The **softmax** function is a more generalized logistic activation function which is used for **multiclass classification**.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "<img src=\"images/tanh.jpeg\" style=\"width: 400px;\"/>\n",
    "\n",
    "$$tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "The range of the tanh function is from **(-1 to 1)**. tanh is also sigmoidal (s - shaped). The advantage is that the negative inputs will be mapped strongly negative **(zero-mean, derivative slope steepest around 0)** and the zeroinputs will be mapped near zero in the tanh graph.\n",
    "\n",
    "The function is **monotonic & differentiable** while its derivative is not monotonic.\n",
    "\n",
    "The tanh function is normally superior to sigmoid at hidden layer.\n",
    "\n",
    "### ReLU & Leaky ReLU\n",
    "\n",
    "<img src=\"images/sigmoid_vs_relu.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "$$ReLU(z) = max(0, z)$$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial ReLU(z)}{\\partial(z)}=\\begin{cases}\n",
    "        0, & \\text{if $x<0$}.\\\\\n",
    "        1, & \\text{otherwise}.\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Non-differentiable on zero is not a big deal for computer (Offen zero in very small demical like $1e^{-10}$).\n",
    "\n",
    "But the issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately. So here is **leaky ReLU** came out.\n",
    "\n",
    "<img src=\"images/relu_vs_leakyrelu.jpeg\" style=\"width: 600px;\"/>\n",
    "\n",
    "<center>Fig : ReLU v/s Leaky ReLU</center>\n",
    "\n",
    "The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.\n",
    "\n",
    "When a is not 0.01 then it is called Randomized ReLU.\n",
    "\n",
    "Therefore the range of the Leaky ReLU is (-infinity to infinity).\n",
    "\n",
    "Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.\n",
    "\n",
    "### Cheetsheet & Derivative\n",
    "\n",
    "<img src=\"images/act_fun_cheetsheet.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "<center>Fig: Activation Function Cheetsheet</center><br>\n",
    "\n",
    "<img src=\"images/af_derivative.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "## CNN\n",
    "## RNN\n",
    "## RL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
